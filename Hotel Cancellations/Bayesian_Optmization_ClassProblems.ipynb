{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div style = \"background-color:   rgba(0, 169, 143, 0.1); padding: 10px;\">\n",
    "\n",
    "### Bayesian Optimization (ONLY FOR CLASSIFICATION PROBLEMS!!!)\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Documentation of Hyperopt Bayesian optimization can be found [Here](https://github.com/hyperopt/hyperopt/wiki/FMin)\n",
    "\n",
    "---\n",
    "\n",
    "**Hyperopt** is a Python library for optimizing machine learning models using Bayesian optimization. **Bayesian optimization** is a probabilistic model-based approach for global optimization of black-box functions. In the context of machine learning, the **black-box function** is typically the performance (objective) of a model on a given task.\n",
    "- **Objective Function (Black-Box Function)**: In machine learning, the objective function is a function that evaluates the performance of a model given a set of hyperparameters. The goal is to find the set of hyperparameters that maximizes or minimizes this objective function.\n",
    "- **Bayesian Optimization**: Bayesian optimization is a probabilistic model-based approach that uses a surrogate model to approximate the true objective function. It maintains a probabilistic model (usually a Gaussian Process) that predicts the distribution of the objective function across the hyperparameter space. The model is updated iteratively as new evaluations of the objective function are obtained.\n",
    "- **Acquisition Function**: The acquisition function is a criterion used to decide where to sample the objective function next. It balances exploration (sampling in regions where uncertainty is high) and exploitation (sampling in regions where the objective function is likely to be optimal). Common acquisition functions include Probability of Improvement (PI), Expected Improvement (EI), and Upper Confidence Bound (UCB).\n",
    "- **Trials and History**: Bayesian optimization is an iterative process. Each iteration consists of evaluating the objective function at a specific set of hyperparameters. These evaluations are referred to as trials. The history of trials is used to update the probabilistic model and make informed decisions about where to sample next.\n",
    "- **Hyperparameter Space**: The hyperparameter space is the range of values that each hyperparameter can take. Hyperopt allows you to define a search space for hyperparameters, specifying the type of each hyperparameter (continuous, discrete, or categorical) and its possible values.\n",
    "- **Optimization Algorithm**: Hyperopt uses a combination of Tree of Parzen Estimators (TPE) and Random Search for global optimization. TPE is a Bayesian optimization algorithm that models the probability of improvement and uses it to guide the search.\n",
    "\n",
    "---\n",
    "\n",
    "There are four key elements for Hyperopt:\n",
    "- The **Space** over which to search\n",
    "- The **Objective Function** to minimize\n",
    "- The **Database** in which to store all the point evaluations of the search\n",
    "- The **Search Algorithm** to use"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Space: The same space as the Random Search\n",
    "space = param_grid_RS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits = 5,                     # the training dataset is split into 5 folds\n",
    "                        shuffle=True,                     # the dataset will be shuffled before splitting into folds. Note that the samples within each split will not be shuffled.\n",
    "                        random_state=0)                   # make the split reproducible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Objective function\n",
    "def objective(params):\n",
    "\n",
    "    xgbreg = XGBRegressor(seed=0, **params)                        # XGBRegressor is used as the model algorithm. seed=0 makes the model results reproducible. **params takes in the hyperparameter values.\n",
    "    score = cross_val_score(estimator = xgbreg,                    # 'cross_val_score' produces k scores, one for each of the k folds. We get the mean of the k scores and output the average value. estimator takes the estimator to fit the data.\n",
    "                            X = X_train,                           # X takes the training dataset feature matrix and\n",
    "                            y = y_train,                           # y takes the target variable for the training dataset.\n",
    "                            cv = kfold,                            # cv determines the cross-validation splitting strategy\n",
    "                            scoring = 'r2',\n",
    "                            n_jobs=-1).mean()                      # n_jobs = -1 enables parallel model training.\n",
    "\n",
    "    # Loss is negative score\n",
    "    loss = - score\n",
    "\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEmpty\u001B[0m                                     Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1415\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m   1414\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1415\u001B[0m     tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ready_batches\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1416\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m queue\u001B[38;5;241m.\u001B[39mEmpty:\n\u001B[0;32m   1417\u001B[0m     \u001B[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001B[39;00m\n\u001B[0;32m   1418\u001B[0m     \u001B[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1421\u001B[0m     \u001B[38;5;66;03m# accordingly to distribute evenly the last items between all\u001B[39;00m\n\u001B[0;32m   1422\u001B[0m     \u001B[38;5;66;03m# workers.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:168\u001B[0m, in \u001B[0;36mQueue.get\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_qsize():\n\u001B[1;32m--> 168\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mEmpty\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Optimize\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m best \u001B[38;5;241m=\u001B[39m \u001B[43mfmin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                                 \u001B[49m\u001B[38;5;66;43;03m# 'fmin' is the function to search the best hyperparameters with the smallest loss value.\u001B[39;49;00m\n\u001B[0;32m      3\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspace\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                                  \u001B[49m\u001B[38;5;66;43;03m# the search space of the hyperparameters.\u001B[39;49;00m\n\u001B[0;32m      4\u001B[0m \u001B[43m            \u001B[49m\u001B[43malgo\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtpe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuggest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                             \u001B[49m\u001B[38;5;66;43;03m# the type of search algorithms. Hyperopt currently has three algorithms, random search, Tree of Parzen Estimators (TPE), and adaptive TPE. Using TPE this time.\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m48\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m                                 \u001B[49m\u001B[38;5;66;43;03m# specifies the maximum number of evaluations.\u001B[39;49;00m\n\u001B[0;32m      6\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtrials\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mTrials\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m                              \u001B[38;5;66;03m# stores the information for the evaluations.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:540\u001B[0m, in \u001B[0;36mfmin\u001B[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[0;32m    537\u001B[0m     fn \u001B[38;5;241m=\u001B[39m __objective_fmin_wrapper(fn)\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_trials_fmin \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(trials, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfmin\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrials\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmin\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m        \u001B[49m\u001B[43malgo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malgo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_queue_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_argmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_argmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progressbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progressbar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m        \u001B[49m\u001B[43mearly_stop_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stop_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrials_save_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrials_save_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    559\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(trials_save_file):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:671\u001B[0m, in \u001B[0;36mTrials.fmin\u001B[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;66;03m# -- Stop-gap implementation!\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;66;03m#    fmin should have been a Trials method in the first place\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m#    but for now it's still sitting in another file.\u001B[39;00m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfmin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fmin\n\u001B[1;32m--> 671\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfmin\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43malgo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malgo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_trials_fmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# -- prevent recursion\u001B[39;49;00m\n\u001B[0;32m    683\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_argmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_argmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshow_progressbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progressbar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stop_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stop_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrials_save_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrials_save_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    689\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001B[0m, in \u001B[0;36mfmin\u001B[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[0;32m    583\u001B[0m rval\u001B[38;5;241m.\u001B[39mcatch_eval_exceptions \u001B[38;5;241m=\u001B[39m catch_eval_exceptions\n\u001B[0;32m    585\u001B[0m \u001B[38;5;66;03m# next line is where the fmin is actually executed\u001B[39;00m\n\u001B[1;32m--> 586\u001B[0m \u001B[43mrval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexhaust\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_argmin:\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(trials\u001B[38;5;241m.\u001B[39mtrials) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001B[0m, in \u001B[0;36mFMinIter.exhaust\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexhaust\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    363\u001B[0m     n_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials)\n\u001B[1;32m--> 364\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mn_done\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_until_done\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masynchronous\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n\u001B[0;32m    366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001B[0m, in \u001B[0;36mFMinIter.run\u001B[1;34m(self, N, block_until_done)\u001B[0m\n\u001B[0;32m    297\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoll_interval_secs)\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;66;03m# -- loop over trials and do the jobs directly\u001B[39;00m\n\u001B[1;32m--> 300\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserial_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials_save_file \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001B[0m, in \u001B[0;36mFMinIter.serial_evaluate\u001B[1;34m(self, N)\u001B[0m\n\u001B[0;32m    176\u001B[0m ctrl \u001B[38;5;241m=\u001B[39m base\u001B[38;5;241m.\u001B[39mCtrl(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials, current_trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 178\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdomain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctrl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    180\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjob exception: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:892\u001B[0m, in \u001B[0;36mDomain.evaluate\u001B[1;34m(self, config, ctrl, attach_attachments)\u001B[0m\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001B[39;00m\n\u001B[0;32m    885\u001B[0m     \u001B[38;5;66;03m#    either into the pyll part (self.expr)\u001B[39;00m\n\u001B[0;32m    886\u001B[0m     \u001B[38;5;66;03m#    or the normal Python part (self.fn)\u001B[39;00m\n\u001B[0;32m    887\u001B[0m     pyll_rval \u001B[38;5;241m=\u001B[39m pyll\u001B[38;5;241m.\u001B[39mrec_eval(\n\u001B[0;32m    888\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpr,\n\u001B[0;32m    889\u001B[0m         memo\u001B[38;5;241m=\u001B[39mmemo,\n\u001B[0;32m    890\u001B[0m         print_node_on_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrec_eval_print_node_on_error,\n\u001B[0;32m    891\u001B[0m     )\n\u001B[1;32m--> 892\u001B[0m     rval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpyll_rval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(rval, (\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39mnumber)):\n\u001B[0;32m    895\u001B[0m     dict_rval \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(rval), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: STATUS_OK}\n",
      "Cell \u001B[1;32mIn[49], line 8\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(params)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mobjective\u001B[39m(params):\n\u001B[0;32m      7\u001B[0m     xgbreg \u001B[38;5;241m=\u001B[39m XGBRegressor(seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)                        \u001B[38;5;66;03m# XGBRegressor is used as the model algorithm. seed=0 makes the model results reproducible. **params takes in the hyperparameter values.\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m     score \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mxgbreg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                    \u001B[49m\u001B[38;5;66;43;03m# 'cross_val_score' produces k scores, one for each of the k folds. We get the mean of the k scores and output the average value. estimator takes the estimator to fit the data.\u001B[39;49;00m\n\u001B[0;32m      9\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mX\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                           \u001B[49m\u001B[38;5;66;43;03m# X takes the training dataset feature matrix and\u001B[39;49;00m\n\u001B[0;32m     10\u001B[0m \u001B[43m                            \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                           \u001B[49m\u001B[38;5;66;43;03m# y takes the target variable for the training dataset.\u001B[39;49;00m\n\u001B[0;32m     11\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkfold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m                            \u001B[49m\u001B[38;5;66;43;03m# cv determines the cross-validation splitting strategy\u001B[39;49;00m\n\u001B[0;32m     12\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmean()                      \u001B[38;5;66;03m# n_jobs = -1 enables parallel model training.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# Loss is negative score\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m score\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001B[0m, in \u001B[0;36mcross_val_score\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[0;32m    560\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[1;32m--> 562\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    564\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    572\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    574\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    575\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    209\u001B[0m         )\n\u001B[0;32m    210\u001B[0m     ):\n\u001B[1;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    221\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[0;32m    306\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    308\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 309\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    322\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    328\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    330\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1942\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1936\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_ref \u001B[38;5;241m=\u001B[39m weakref\u001B[38;5;241m.\u001B[39mref(output)\n\u001B[0;32m   1938\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1939\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1940\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1941\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m-> 1942\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m   1944\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1580\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1578\u001B[0m detach_generator_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1579\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_start\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1581\u001B[0m     \u001B[38;5;66;03m# first yield returns None, for internal use only. This ensures\u001B[39;00m\n\u001B[0;32m   1582\u001B[0m     \u001B[38;5;66;03m# that we enter the try/except block and start dispatching the\u001B[39;00m\n\u001B[0;32m   1583\u001B[0m     \u001B[38;5;66;03m# tasks.\u001B[39;00m\n\u001B[0;32m   1584\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1563\u001B[0m, in \u001B[0;36mParallel._start\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_start\u001B[39m(\u001B[38;5;28mself\u001B[39m, iterator, pre_dispatch):\n\u001B[0;32m   1555\u001B[0m     \u001B[38;5;66;03m# Only set self._iterating to True if at least a batch\u001B[39;00m\n\u001B[0;32m   1556\u001B[0m     \u001B[38;5;66;03m# was dispatched. In particular this covers the edge\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1560\u001B[0m     \u001B[38;5;66;03m# was very quick and its callback already dispatched all the\u001B[39;00m\n\u001B[0;32m   1561\u001B[0m     \u001B[38;5;66;03m# remaining jobs.\u001B[39;00m\n\u001B[0;32m   1562\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1563\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1564\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1426\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m   1423\u001B[0m n_jobs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cached_effective_n_jobs\n\u001B[0;32m   1424\u001B[0m big_batch_size \u001B[38;5;241m=\u001B[39m batch_size \u001B[38;5;241m*\u001B[39m n_jobs\n\u001B[1;32m-> 1426\u001B[0m islice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(itertools\u001B[38;5;241m.\u001B[39mislice(iterator, big_batch_size))\n\u001B[0;32m   1427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(islice) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:61\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# in a different thread depending on the backend and on the value of\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m# pre_dispatch and n_jobs.\u001B[39;00m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m---> 61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    306\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    308\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 309\u001B[0m results \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[0;32m    310\u001B[0m     delayed(_fit_and_score)(\n\u001B[0;32m    311\u001B[0m         clone(estimator),\n\u001B[0;32m    312\u001B[0m         X,\n\u001B[0;32m    313\u001B[0m         y,\n\u001B[0;32m    314\u001B[0m         scorers,\n\u001B[0;32m    315\u001B[0m         train,\n\u001B[0;32m    316\u001B[0m         test,\n\u001B[0;32m    317\u001B[0m         verbose,\n\u001B[0;32m    318\u001B[0m         \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    319\u001B[0m         fit_params,\n\u001B[0;32m    320\u001B[0m         return_train_score\u001B[38;5;241m=\u001B[39mreturn_train_score,\n\u001B[0;32m    321\u001B[0m         return_times\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    322\u001B[0m         return_estimator\u001B[38;5;241m=\u001B[39mreturn_estimator,\n\u001B[0;32m    323\u001B[0m         error_score\u001B[38;5;241m=\u001B[39merror_score,\n\u001B[0;32m    324\u001B[0m     )\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m indices\n\u001B[0;32m    326\u001B[0m )\n\u001B[0;32m    328\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    330\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:377\u001B[0m, in \u001B[0;36m_BaseKFold.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits \u001B[38;5;241m>\u001B[39m n_samples:\n\u001B[0;32m    370\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    371\u001B[0m         (\n\u001B[0;32m    372\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot have number of splits n_splits=\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m greater\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    373\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m than the number of samples: n_samples=\u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    374\u001B[0m         )\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits, n_samples)\n\u001B[0;32m    375\u001B[0m     )\n\u001B[1;32m--> 377\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msplit(X, y, groups):\n\u001B[0;32m    378\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:108\u001B[0m, in \u001B[0;36mBaseCrossValidator.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m    106\u001B[0m X, y, groups \u001B[38;5;241m=\u001B[39m indexable(X, y, groups)\n\u001B[0;32m    107\u001B[0m indices \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(_num_samples(X))\n\u001B[1;32m--> 108\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m test_index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_test_masks(X, y, groups):\n\u001B[0;32m    109\u001B[0m     train_index \u001B[38;5;241m=\u001B[39m indices[np\u001B[38;5;241m.\u001B[39mlogical_not(test_index)]\n\u001B[0;32m    110\u001B[0m     test_index \u001B[38;5;241m=\u001B[39m indices[test_index]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:758\u001B[0m, in \u001B[0;36mStratifiedKFold._iter_test_masks\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m    757\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_iter_test_masks\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 758\u001B[0m     test_folds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_test_folds\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    759\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits):\n\u001B[0;32m    760\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m test_folds \u001B[38;5;241m==\u001B[39m i\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:701\u001B[0m, in \u001B[0;36mStratifiedKFold._make_test_folds\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    699\u001B[0m allowed_target_types \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    700\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m type_of_target_y \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m allowed_target_types:\n\u001B[1;32m--> 701\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    702\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSupported target types are: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    703\u001B[0m             allowed_target_types, type_of_target_y\n\u001B[0;32m    704\u001B[0m         )\n\u001B[0;32m    705\u001B[0m     )\n\u001B[0;32m    707\u001B[0m y \u001B[38;5;241m=\u001B[39m column_or_1d(y)\n\u001B[0;32m    709\u001B[0m _, y_idx, y_inv \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y, return_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_inverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mValueError\u001B[0m: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead."
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "best = fmin(fn = objective,                                 # 'fmin' is the function to search the best hyperparameters with the smallest loss value.\n",
    "            space = space,                                  # the search space of the hyperparameters.\n",
    "            algo = tpe.suggest,                             # the type of search algorithms. Hyperopt currently has three algorithms, random search, Tree of Parzen Estimators (TPE), and adaptive TPE. Using TPE this time.\n",
    "            max_evals = 48,                                 # specifies the maximum number of evaluations.\n",
    "            trials = Trials())                              # stores the information for the evaluations."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
