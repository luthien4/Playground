{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"color:white; background-color: black; padding: 20px; border-radius:8px; font-size:26px\"><b style=\"font-weight: 700;\"><center>Coronavirus Tweets - Text Classification</center></b></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color:  #eddcd2; padding: 10px;\">\n",
    "\n",
    "### The Data\n",
    "\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data collected from [here](https://www.kaggle.com/datasets/lakshmi25npathi/coronavirus-tweets-dataset)\n",
    "\n",
    "**About this Dataset**:\n",
    "\n",
    "Perform Text Classification on the data. The tweets have been pulled from Twitter and manual tagging has been done then.\n",
    "The names and usernames have been given codes to avoid any privacy concerns.\n",
    "\n",
    "Columns:\n",
    "1) Location\n",
    "2) Tweet At\n",
    "3) Original Tweet\n",
    "4) Label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"background-color:  #eddcd2; padding: 10px;\">\n",
    "\n",
    "### Text Classification\n",
    "\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To predict the number of positive and negative tweets using:\n",
    " - **Classification ML algorithms**\n",
    " - **Deep learning algorithms**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Brief Summary of regular expression functions**\n",
    "\n",
    "- `re.compile()`: It is used to compile a regular expression pattern into a regular expression object. This compiled pattern can be reused for multiple operations, making it more efficient, especially when working with complex or frequently used regular expressions. Benefits of re.compile():\n",
    "    - *Improved Efficiency*: It can improve the performance of your code, especially if you are using the same regular expression in multiple places.\n",
    "    - *Readability*: It can make your code more readable by separating the pattern definition from its usage.\n",
    "    - *Facilitates Code Maintenance*: If you need to modify the regular expression later, you only have to do it in one place (where it's compiled), rather than multiple places where it's used.\n",
    "    - *Pre-Compile Complex Patterns*: For complex regular expressions, pre-compiling can help manage the complexity and make your code more organized.\n",
    "---\n",
    "\n",
    "- `re.sub(pattern, replacement, string)`: It performs a search-and-replace operation where:\n",
    "    - `pattern`: The regular expression pattern to search for in the string.\n",
    "    - `replacement`: The string to replace the matched patterns.\n",
    "    - `string`: The input string where the search and replace operation will be performed.\n",
    "\n",
    "---\n",
    "\n",
    "- `re.findall(pattern, string, flags=0)`: It is a function in Python's **re** module (regular expressions) that allows you to find all non-overlapping occurrences of a pattern in a string. It returns a list of all the matching substrings or elements.\n",
    "    - `pattern`: This is the regular expression pattern you want to search for in the given string.\n",
    "    - `string`: The input string where you want to search for matches.\n",
    "    - `flags`: Optional. Flags can be used to modify the behavior of the regular expression. For example, you can use re.IGNORECASE to perform a case-insensitive search."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "\n",
    "## TABLE OF CONTENTS\n",
    "\n",
    "- <a href='#1'>1. IMPORTING LIBRARIES</a>\n",
    "- <a href='#2'>2. READING DATA</a>\n",
    "- <a href='#3'>3. DATA CLEANING</a>\n",
    "    - <a href='#3-1'>3.1 Lowercasing</a>\n",
    "    - <a href='#3-2'>3.2 Remove HTML Tags</a>\n",
    "    - <a href='#3-3'>3.3 Remove URLs</a>\n",
    "    - <a href='#3-4'>3.4 Remove Punctuation</a>\n",
    "    - <a href='#3-5'>3.5 Chat word treatment</a>\n",
    "    - <a href='#3-6'>3.6 Spelling Correction</a>\n",
    "    - <a href='#3-7'>3.7 Handling Emojis</a>\n",
    "- <a href='#4'>4. TEXT PREPROCESSING</a>\n",
    "    - <a href='#4-1'>4.1 Remove Stop Words</a>\n",
    "    - <a href='#4-2'>4.2 Tokenization</a>\n",
    "    - <a href='#4-3'>4.3 Stemming</a>\n",
    "    - <a href='#4-4'>4.4 Lemmatization</a>\n",
    "- <a href='#5'>5. DATA SPLITTING</a>\n",
    "- <a href='#6'>6. FEATURE EXTRACTION FROM TEXT</a>\n",
    "    - <a href='#6-1'>6.1 One Hot Encoding</a>\n",
    "    - <a href='#6-2'>6.2 Bag of Words</a>\n",
    "    - <a href='#6-3'>6.3 N-grams</a>\n",
    "    - <a href='#6-4'>6.4 Term frequency - Inverse document frequency </a>\n",
    "    - <a href='#6-5'>6.5 Word2Vec </a>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a id='1'>1. Importing Libraries </a>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np                          # for working with arrays and matrices\n",
    "\n",
    "pd.set_option('display.max_rows', 500)      # Set max number of rows displayed\n",
    "pd.set_option('display.max_columns', 500)   # Set max number of columns displayed\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Regex pkg\n",
    "import re\n",
    "\n",
    "# String and time module\n",
    "import string, time\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt             # for creating plots\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "\n",
    "# Split pkgs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import skew\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Save and load pkgs\n",
    "from pickle import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T21:00:17.349701500Z",
     "start_time": "2023-10-15T21:00:17.318291100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a id='2'>2. Reading Data </a>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   UserName  ScreenName                  Location     TweetAt                                      OriginalTweet           Sentiment\n0      3799       48751                    London  16-03-2020  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n1      3800       48752                        UK  16-03-2020  advice Talk to your neighbours family to excha...            Positive\n2      3801       48753                 Vagabonds  16-03-2020  Coronavirus Australia: Woolworths to give elde...            Positive\n3      3802       48754                       NaN  16-03-2020  My food stock is not the only one which is emp...            Positive\n4      3803       48755                       NaN  16-03-2020  Me, ready to go at supermarket during the #COV...  Extremely Negative\n5      3804       48756  ÜT: 36.319708,-82.363649  16-03-2020  As news of the region_x0092_s first confirmed ...            Positive\n6      3805       48757      35.926541,-78.753267  16-03-2020  Cashier at grocery store was sharing his insig...            Positive\n7      3806       48758                   Austria  16-03-2020  Was at the supermarket today. Didn't buy toile...             Neutral\n8      3807       48759           Atlanta, GA USA  16-03-2020  Due to COVID-19 our retail store and classroom...            Positive\n9      3808       48760          BHAVNAGAR,GUJRAT  16-03-2020  For corona prevention,we should stop to buy th...            Negative",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>advice Talk to your neighbours family to excha...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>Coronavirus Australia: Woolworths to give elde...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>My food stock is not the only one which is emp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>Me, ready to go at supermarket during the #COV...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÜT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>As news of the region_x0092_s first confirmed ...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>Cashier at grocery store was sharing his insig...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>Was at the supermarket today. Didn't buy toile...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>Due to COVID-19 our retail store and classroom...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>For corona prevention,we should stop to buy th...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41157, 6)\n",
      "UserName                                                      3800\n",
      "ScreenName                                                   48752\n",
      "Location                                                        UK\n",
      "TweetAt                                                 16-03-2020\n",
      "OriginalTweet    advice Talk to your neighbours family to excha...\n",
      "Sentiment                                                 Positive\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df = pd.read_excel(\"D:/Data Science/Datasets/NLP/Covid_tweets/Corona_NLP_train.xlsx\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "print(df.iloc[1,:])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.861368400Z",
     "start_time": "2023-10-14T21:33:06.876798400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"My food stock is not the only one which is empty...\\n\\n\\n\\nPLEASE, don't panic, THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need. \\n\\nStay calm, stay safe.\\n\\n\\n\\n#COVID19france #COVID_19 #COVID19 #coronavirus #confinement #Confinementotal #ConfinementGeneral https://t.co/zrlG0Z520j\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OriginalTweet'][3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.871927800Z",
     "start_time": "2023-10-14T21:33:09.861368400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a id='3'>3. Data Cleaning </a>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-1'>3.1 Lowercasing</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "\"my food stock is not the only one which is empty...\\n\\n\\n\\nplease, don't panic, there will be enough food for everyone if you do not take more than you need. \\n\\nstay calm, stay safe.\\n\\n\\n\\n#covid19france #covid_19 #covid19 #coronavirus #confinement #confinementotal #confinementgeneral https://t.co/zrlg0z520j\""
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OriginalTweet'][3].lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.871927800Z",
     "start_time": "2023-10-14T21:33:09.870934900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.883140800Z",
     "start_time": "2023-10-14T21:33:09.880959200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "       UserName  ScreenName                      Location     TweetAt                                      OriginalTweet           Sentiment\n0          3799       48751                        London  16-03-2020  @menyrbie @phil_gahan @chrisitv https://t.co/i...             Neutral\n1          3800       48752                            UK  16-03-2020  advice talk to your neighbours family to excha...            Positive\n2          3801       48753                     Vagabonds  16-03-2020  coronavirus australia: woolworths to give elde...            Positive\n3          3802       48754                           NaN  16-03-2020  my food stock is not the only one which is emp...            Positive\n4          3803       48755                           NaN  16-03-2020  me, ready to go at supermarket during the #cov...  Extremely Negative\n...         ...         ...                           ...         ...                                                ...                 ...\n41152     44951       89903  Wellington City, New Zealand  14-04-2020  airline pilots offering to stock supermarket s...             Neutral\n41153     44952       89904                           NaN  14-04-2020  response to complaint not provided citing covi...  Extremely Negative\n41154     44953       89905                           NaN  14-04-2020  you know it_x0092_s getting tough when @kamero...            Positive\n41155     44954       89906                           NaN  14-04-2020  is it wrong that the smell of hand sanitizer i...             Neutral\n41156     44955       89907  i love you so much || he/him  14-04-2020  @tartiicat well new/used rift s are going for ...            Negative\n\n[41157 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>@menyrbie @phil_gahan @chrisitv https://t.co/i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>advice talk to your neighbours family to excha...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>coronavirus australia: woolworths to give elde...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>my food stock is not the only one which is emp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>me, ready to go at supermarket during the #cov...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41152</th>\n      <td>44951</td>\n      <td>89903</td>\n      <td>Wellington City, New Zealand</td>\n      <td>14-04-2020</td>\n      <td>airline pilots offering to stock supermarket s...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41153</th>\n      <td>44952</td>\n      <td>89904</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>response to complaint not provided citing covi...</td>\n      <td>Extremely Negative</td>\n    </tr>\n    <tr>\n      <th>41154</th>\n      <td>44953</td>\n      <td>89905</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>you know it_x0092_s getting tough when @kamero...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>41155</th>\n      <td>44954</td>\n      <td>89906</td>\n      <td>NaN</td>\n      <td>14-04-2020</td>\n      <td>is it wrong that the smell of hand sanitizer i...</td>\n      <td>Neutral</td>\n    </tr>\n    <tr>\n      <th>41156</th>\n      <td>44955</td>\n      <td>89907</td>\n      <td>i love you so much || he/him</td>\n      <td>14-04-2020</td>\n      <td>@tartiicat well new/used rift s are going for ...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>41157 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.892856600Z",
     "start_time": "2023-10-14T21:33:09.884136500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-2'>3.2 Remove HTML Tags</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "\n",
    "    # Define the regex pattern to match HTML tags\n",
    "    pattern = re.compile('<.*?>')\n",
    "\n",
    "    # Use re.sub() to remove HTML tags from the text\n",
    "    cleaned_text = pattern.sub(r'', text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.940792800Z",
     "start_time": "2023-10-14T21:33:09.892856600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(remove_html_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:09.985785300Z",
     "start_time": "2023-10-14T21:33:09.897341500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-3'>3.3 Remove URLs </a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    cleaned_text = pattern.sub(r'', text)\n",
    "\n",
    "    return cleaned_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.031633300Z",
     "start_time": "2023-10-14T21:33:09.919043900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(remove_url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.093935400Z",
     "start_time": "2023-10-14T21:33:09.923345100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-4'>3.4 Remove Punctuation </a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation             # <--- contains a collection of punctuation characters"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.093935400Z",
     "start_time": "2023-10-14T21:33:09.985785300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "\n",
    "    exclude = string.punctuation\n",
    "\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')       # <--- It replaces the punctuation with whitespace in text\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.093935400Z",
     "start_time": "2023-10-14T21:33:09.985785300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(remove_punc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.132044900Z",
     "start_time": "2023-10-14T21:33:09.985785300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'with 100  nations inficted with  covid  19  the world must  not  play fair with china  100 goverments must demand  china  adopts new guilde  lines on food safty  the  chinese  goverment  is guilty of  being  irosponcible   with life  on a global scale'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OriginalTweet'][20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.133041200Z",
     "start_time": "2023-10-14T21:33:10.106455500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-5'>3.5 Chat Word Treatment </a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Chat word treatment* refers to the handling or processing of abbreviated or informal language commonly used in online chat conversations, social media posts, and text messaging. It involves converting or normalizing these abbreviated forms into their full, standard English equivalents.\n",
    "\n",
    "- Examples of abbreviated or informal language: *lmao*, *imho*, *fyi*, *asap*, *gn*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "chat_words = {'AFAIK': 'As Far As I Know',\n",
    "              'AFK': 'Away From Keyboard',\n",
    "              'ASAP': 'As Soon As Possible',\n",
    "              'ATK': 'At The Keyboard',\n",
    "              'ATM': 'At The Moment',\n",
    "              'A3': 'Anytime, Anywhere, Anyplace',\n",
    "              'BAK': 'Back At Keyboard',\n",
    "              'BBL': 'Be Back Later',\n",
    "              'BBS': 'Be Back Soon',\n",
    "              'BFN': 'Bye For Now',\n",
    "              'B4N': 'Bye For Now',\n",
    "              'BRB': 'Be Right Back',\n",
    "              'BRT': 'Be Right There',\n",
    "              'BTW': 'By The Way',\n",
    "              'B4': 'Before',\n",
    "              'CU': 'See You',\n",
    "              'CUL8R': 'See You Later',\n",
    "              'CYA': 'See You',\n",
    "              'FAQ': 'Frequently Asked Questions',\n",
    "              'FC': 'Fingers Crossed',\n",
    "              'FWIW': \"For What It's Worth\",\n",
    "              'FYI': 'For Your Information',\n",
    "              'GAL': 'Get A Life',\n",
    "              'GG': 'Good Game',\n",
    "              'GN': 'Good Night',\n",
    "              'GMTA': 'Great Minds Think Alike',\n",
    "              'GR8': 'Great!',\n",
    "              'G9': 'Genius',\n",
    "              'IC': 'I See',\n",
    "              'ICQ': 'I Seek you (also a chat program)',\n",
    "              'ILU': 'I Love You',\n",
    "              'IMHO': 'In My Honest/Humble Opinion',\n",
    "              'IMO': 'In My Opinion',\n",
    "              'IOW': 'In Other Words',\n",
    "              'IRL': 'In Real Life',\n",
    "              'KISS': 'Keep It Simple, Stupid',\n",
    "              'LDR': 'Long Distance Relationship',\n",
    "              'LMAO': 'Laugh My A.. Off',\n",
    "              'LOL': 'Laughing Out Loud',\n",
    "              'LTNS': 'Long Time No See',\n",
    "              'L8R': 'Later',\n",
    "              'MTE': 'My Thoughts Exactly',\n",
    "              'M8': 'Mate',\n",
    "              'NRN': 'No Reply Necessary',\n",
    "              'OIC': 'Oh I See',\n",
    "              'PITA': 'Pain In The A..',\n",
    "              'PRT': 'Party',\n",
    "              'PRW': 'Parents Are Watching',\n",
    "              'QPSA?':\t'Que Pasa?',\n",
    "              'ROFL': 'Rolling On The Floor Laughing',\n",
    "              'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "              'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "              'SK8': 'Skate',\n",
    "              'STATS': 'Your sex and age',\n",
    "              'ASL': 'Age, Sex, Location',\n",
    "              'THX': 'Thank You',\n",
    "              'TTFN': 'Ta-Ta For Now!',\n",
    "              'TTYL': 'Talk To You Later',\n",
    "              'U': 'You',\n",
    "              'U2': 'You Too',\n",
    "              'U4E': 'Yours For Ever',\n",
    "              'WB': 'Welcome Back',\n",
    "              'WTF': 'What The F...',\n",
    "              'WTG': 'Way To Go!',\n",
    "              'WUF': 'Where Are You From?',\n",
    "              'W8': 'Wait...',\n",
    "              '7K': 'Sick',\n",
    "              ':-D': 'Laugher',\n",
    "              'TFW': 'That feeling when',\n",
    "              'MFW': 'My face when',\n",
    "              'MRW': 'My reaction when',\n",
    "              'IFYP': 'I feel your pain',\n",
    "              'TNTL': 'Trying not to laugh',\n",
    "              'JK': 'Just kidding',\n",
    "              'IDC': 'I don’t care',\n",
    "              'ILY': 'I love you',\n",
    "              'IMU': 'I miss you',\n",
    "              'ADIH': 'Another day in hell',\n",
    "              'ZZZ': 'Sleeping, bored, tired',\n",
    "              'WYWH': 'Wish you were here',\n",
    "              'TIME': 'Tears in my eyes',\n",
    "              'BAE': 'Before anyone else',\n",
    "              'FIMH': 'Forever in my heart',\n",
    "              'BSAAW': 'Big smile and a wink',\n",
    "              'BWL': 'Bursting with laughter',\n",
    "              'BFF': 'Best friends forever',\n",
    "              'CSL': 'Can’t stop laughing'\n",
    "              }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.133041200Z",
     "start_time": "2023-10-14T21:33:10.112098900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "\n",
    "    new_text = []\n",
    "\n",
    "    for w in text.split():                               # <--- for each word in text\n",
    "        if w.upper() in chat_words:                      # <--- checking if the word (in capital letters) is a key of chat_words\n",
    "            new_text.append(chat_words[w.upper()])       # <--- adding the extended text of the chat word\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "\n",
    "    return ' '.join(new_text)                            # <--- return \"text\" with the \"new_text\" instead of chat words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.133041200Z",
     "start_time": "2023-10-14T21:33:10.114851400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n",
      "For Your Information La Habana is the capital of Cuba\n"
     ]
    }
   ],
   "source": [
    "print(chat_conversion('IMHO he is the best'))\n",
    "print(chat_conversion('FYI La Habana is the capital of Cuba'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.133041200Z",
     "start_time": "2023-10-14T21:33:10.118150400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0                      menyrbie philgahan chrisitv and and\n1        advice talk to your neighbours family to excha...\n2        coronavirus australia woolworths to give elder...\n3        my food stock is not the only one which is emp...\n4        me ready to go at supermarket during the covid...\n                               ...                        \n41152    airline pilots offering to stock supermarket s...\n41153    response to complaint not provided citing covi...\n41154    you know itx0092s getting tough when kameronwi...\n41155    is it wrong that the smell of hand sanitizer i...\n41156    tartiicat well newused rift s are going for 70...\nName: OriginalTweet, Length: 41157, dtype: object"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OriginalTweet'].apply(chat_conversion)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.272639Z",
     "start_time": "2023-10-14T21:33:10.121081800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(chat_conversion)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.418666600Z",
     "start_time": "2023-10-14T21:33:10.272639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-6'>3.6 Spelling Correction (too long running time!)</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TextBlob** is a Python library that provides a simple API for common natural language processing (NLP) tasks. It's built on top of NLTK (Natural Language Toolkit) and Pattern. It abstracts away many of the complexities of lower-level NLP tasks, allowing users to perform, among others, the following tasks:\n",
    "\n",
    "- *Sentiment Analysis*: **TextBlob** can determine the sentiment (polarity and subjectivity) of a piece of text. It can tell you whether a text expresses a positive, negative, or neutral sentiment.\n",
    "\n",
    "- *Part-of-Speech Tagging*: **TextBlob** can identify the grammatical parts of a sentence, such as nouns, verbs, adjectives, etc.\n",
    "\n",
    "- *Noun Phrase Extraction*: It can extract noun phrases from text. This is useful for tasks like information extraction.\n",
    "\n",
    "- *Translation and Language Detection*: **TextBlob** supports translation between different languages and can detect the language of a given text.\n",
    "\n",
    "- *Word Inflection and Lemmatization*: It can convert words into their base or root form, which can be helpful for tasks like text classification.\n",
    "\n",
    "- *Spell Checking*: **TextBlob** can correct spelling mistakes in a given text.\n",
    "\n",
    "- *Tokenization*: It can break a text into individual words or sentences, a necessary step for many NLP tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(41157, 6)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:10.423665300Z",
     "start_time": "2023-10-14T21:33:10.419663300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.610276700Z",
     "start_time": "2023-10-14T21:33:10.423665300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def spelling_correction(text, exceptions):\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize an empty list to store corrected words\n",
    "    corrected_words = []\n",
    "\n",
    "    # Correct each word, except for the specified exception\n",
    "    for word in words:\n",
    "        if word in exceptions:\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "\n",
    "            # Create a TextBlob object using the word\n",
    "            textBlb = TextBlob(word)\n",
    "\n",
    "            corrected_words.append(textBlb.correct().string)         # <--- The correct() method attempts to correct the spelling and grammar of the text. The string() method converts the corrected text in a string\n",
    "\n",
    "    # Join the corrected words back into a sentence\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "\n",
    "    return corrected_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.614223900Z",
     "start_time": "2023-10-14T21:33:11.611273300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 100 nations infected with covid 19 the world must not play fair with china 100 governments must demand china adopt new guide lines on food safety the chinese government is guilty of being irosponcible with life on a global scale\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "incorrect_text = 'with 100  nations inficted with  covid  19  the world must  not  play fair with china  100 goverments must demand  china  adopts new guilde  lines on food safty  the  chinese  goverment  is guilty of  being  irosponcible   with life  on a global scale'\n",
    "\n",
    "print(spelling_correction(incorrect_text, ['covid', 'covid19']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.865639400Z",
     "start_time": "2023-10-14T21:33:11.614223900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# df['OriginalTweet'] = df['OriginalTweet'].apply(spelling_correction, ['covid', 'covid19'])\n",
    "# end = time.time() - start\n",
    "# print(end)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.891454700Z",
     "start_time": "2023-10-14T21:33:11.864585100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='3-7'>3.7 Handling Emojis</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Emojis can be:\n",
    "- Removed\n",
    "- Replaced by its meaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove emojis (also other symbols):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "\n",
    "    emoji_pattern = re.compile('['\n",
    "                               u\"\\U0001F600-\\U0001F64F\"     # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"     # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"     # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"     # flags (iOS)\n",
    "                               u'\\U0001F700-\\U0001F77F'\n",
    "                               u'\\U0001F780-\\U0001F7FF'\n",
    "                               u'\\U0001F800-\\U0001F8FF'\n",
    "                               u'\\U0001F900-\\U0001F9FF'\n",
    "                               u'\\U0001FA00-\\U0001FA6F'\n",
    "                               u'\\U0001FA70-\\U0001FAFF'\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               ']+', flags = re.UNICODE)\n",
    "\n",
    "    new_text = emoji_pattern.sub(r'', text)\n",
    "    return new_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.907400100Z",
     "start_time": "2023-10-14T21:33:11.866637500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was 😍🤩 \n",
      "Loved the movie. It was  \n"
     ]
    }
   ],
   "source": [
    "text = 'Loved the movie. It was \\U0001F60D\\U0001F929 '\n",
    "print(text)\n",
    "\n",
    "print(remove_emoji(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.945799700Z",
     "start_time": "2023-10-14T21:33:11.891454700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replace emojis by its meaning (Problem with emoji-Django module):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# import emoji\n",
    "#\n",
    "# print(emoji.demojize('Python is 🔥'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.946796400Z",
     "start_time": "2023-10-14T21:33:11.907400100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a id='4'>4. Text Preprocessing </a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='4-1'>4.1 Removing Stop words</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T19:55:55.536746400Z",
     "start_time": "2023-10-15T19:55:55.533756400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.946796400Z",
     "start_time": "2023-10-14T21:33:11.907400100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "\n",
    "    wordss = text.split()\n",
    "\n",
    "    new_text = []\n",
    "\n",
    "    for word in wordss:\n",
    "\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "\n",
    "    text_nostopwords = ' '.join(x)\n",
    "\n",
    "    return text_nostopwords\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:33:11.946796400Z",
     "start_time": "2023-10-14T21:33:11.907400100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hate grocery shopping in general but i swear ix0092m doing it online next shop can not deal with the swathes of panic buyers at all covid19 coronavirus coronavirusuk anxiety panicbuyinguk morons\n"
     ]
    }
   ],
   "source": [
    "text = df['OriginalTweet'][100]\n",
    "print(text)\n",
    "\n",
    "df['OriginalTweet'] = df['OriginalTweet'].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:36:51.567367100Z",
     "start_time": "2023-10-14T21:35:30.924551900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='4-2'>4.2 Tokenization</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tokenization** is a process in Natural Language Processing (NLP) that involves breaking a text into individual units, or \"tokens.\" These tokens can be *words*, *sentences*, or even *subwords*, depending on the level of granularity required for the specific NLP task.\n",
    "\n",
    "Tokenization is a crucial preprocessing step in NLP pipelines because it converts raw text into a format that can be processed by machines. It enables the extraction of meaningful information and features from text, making it suitable for various NLP tasks such as sentiment analysis, part-of-speech tagging, named entity recognition, and more. Moreover, tokenization is not limited to English; it's a universal concept that applies to text in any language. Different languages may have specific tokenization rules based on their unique linguistic characteristics.\n",
    "\n",
    "Depending on the type of project under analysis, there are three common types of tokenization:\n",
    "\n",
    "- *Word Tokenization*:\n",
    "    - This type of tokenization breaks a text into individual words. For example, the sentence \"Chatbots are amazing!\" would be tokenized into the following words: \"Chatbots\", \"are\", \"amazing\", and \"!\". Punctuation marks are usually treated as separate tokens.\n",
    "    - Word tokenization is one of the most fundamental steps in NLP, as many subsequent NLP tasks rely on individual words.\n",
    "\n",
    "- *Sentence Tokenization*:\n",
    "    - Sentence tokenization involves splitting a text into individual sentences. For example, the paragraph \"This is the first sentence. This is the second sentence.\" would be tokenized into two sentences: \"This is the first sentence.\" and \"This is the second sentence.\"\n",
    "    - Sentence tokenization is important for tasks that require analysis at the sentence level, such as sentiment analysis or machine translation.\n",
    "\n",
    "- *Subword Tokenization*:\n",
    "    - Subword tokenization breaks words into smaller units, which may be meaningful in some languages or for specific tasks. For instance, \"chatbots\" might be tokenized into \"chat\" and \"bots\". This is particularly useful for handling rare or out-of-vocabulary words.\n",
    "    - Subword tokenization is often used in tasks like machine translation and text generation.\n",
    "\n",
    "**Notes**:\n",
    "- *Prefix*: Character(s) at the beginning ---> $(\"\n",
    "- *Suffix*: Character(s) at the end ---> km),.!?\"\n",
    "- *Infix*: Character(s) in between ---> - -- / ...\n",
    "- *Exception*: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied ---> let's U.S."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Using the split function**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'am', 'going', 'to', 'La', 'Habana']"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "text1 = 'I am going to La Habana'\n",
    "text1.split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:34:33.419778300Z",
     "start_time": "2023-10-14T21:34:33.415827600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "['I am going to La Habana',\n ' I will stay for 3 days',\n \" Let's hope the trip to be great\"]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "text2 = 'I am going to La Habana. I will stay for 3 days. Let\\'s hope the trip to be great'\n",
    "text2.split('.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:34:33.424490600Z",
     "start_time": "2023-10-14T21:34:33.420775Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Split function in Tweet data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "0                      [menyrbie, philgahan, chrisitv]\n1    [advice, talk, neighbours, family, exchange, p...\n2    [coronavirus, australia, woolworths, give, eld...\n3    [food, stock, one, empty, please, dont, panic,...\n4    [ready, go, supermarket, covid19, outbreak, im...\nName: OriginalTweet, dtype: object"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets = df['OriginalTweet'].apply(lambda x: x.split())\n",
    "\n",
    "tokenized_tweets.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:29:54.284120500Z",
     "start_time": "2023-10-15T11:29:54.104460200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Using Regular Expression**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "['I', 'am', 'going', 'to', 'La', 'Habana']"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "text3 = 'I am going to La Habana'\n",
    "tokens = re.findall(\"[\\w']+\", text3)\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:37:41.222360600Z",
     "start_time": "2023-10-14T21:37:41.218863400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "['With these selections, players will take the reins of the Japanese Empire',\n 'With dense rainforests to the north, mountains to the west, and the sea to the south, enemies will find it massively difficult to approach one`s starting location',\n 'It`s a strong early-game position with a lot of potential to grow in peace',\n 'Players won`t be left wanting for resources on this map, as there are a plethora of useful goodies scattered close together',\n 'Initial access to the sea won`t be a problem either',\n 'There`s also a nearby Natural Wonder that can be claimed in the first few turns, granting bonuses to adjacent tiles.']"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "text4 = \"\"\"With these selections, players will take the reins of the Japanese Empire? With dense rainforests to the north, mountains to the west, and the sea to the south, enemies will find it massively difficult to approach one`s starting location. It`s a strong early-game position with a lot of potential to grow in peace. Players won`t be left wanting for resources on this map, as there are a plethora of useful goodies scattered close together. Initial access to the sea won`t be a problem either. There`s also a nearby Natural Wonder that can be claimed in the first few turns, granting bonuses to adjacent tiles.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text4)\n",
    "sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-14T21:46:36.618314800Z",
     "start_time": "2023-10-14T21:46:36.614325Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Regex tokenization in Tweet data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "0                      [menyrbie, philgahan, chrisitv]\n1    [advice, talk, neighbours, family, exchange, p...\n2    [coronavirus, australia, woolworths, give, eld...\n3    [food, stock, one, empty, please, dont, panic,...\n4    [ready, go, supermarket, covid19, outbreak, im...\nName: OriginalTweet, dtype: object"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets = df['OriginalTweet'].apply(lambda x: re.findall(\"[\\w']+\", x))\n",
    "\n",
    "tokenized_tweets.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:39:03.675600500Z",
     "start_time": "2023-10-15T11:39:03.519883600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Using NLTK**\n",
    "\n",
    "**NLTK** stands for **Natural Language Toolkit**. It's a comprehensive library in Python that provides tools to work with human language data (text). It's particularly useful for tasks related to linguistic analysis and natural language processing (**NLP**).\n",
    "In the context of tokenization, NLTK provides a module called `nltk.tokenize` which contains various tokenizers for splitting text into tokens. Some of the tokenizers available in NLTK include:\n",
    "\n",
    "- *Word Tokenizer* (`nltk.tokenize.word_tokenize`): This tokenizer breaks text into individual words. It's particularly useful for tasks where you need to analyze the text at the word level.\n",
    "<br>\n",
    "\n",
    "- Sentence Tokenizer* (`nltk.tokenize.sent_tokenize`): This tokenizer splits text into sentences. It's used when you want to analyze text at the sentence level, such as in tasks like sentiment analysis.\n",
    "<br>\n",
    "\n",
    "- *Whitespace Tokenizer* (`nltk.tokenize.WhitespaceTokenizer`): This tokenizer splits text based on whitespace characters (spaces, tabs, etc.). It can be useful for specific cases where whitespace serves as a clear delimiter.\n",
    "<br>\n",
    "\n",
    "- *Regexp Tokenizer* (`nltk.tokenize.RegexpTokenizer`): This tokenizer allows you to define your own custom tokenization rules using regular expressions. It's highly flexible and can be tailored to specific tokenization needs.\n",
    "<br>\n",
    "\n",
    "- *Treebank Tokenizer* (`nltk.tokenize.TreebankWordTokenizer`): This is a tokenizer that is designed to work with the Penn Treebank dataset, which is a large corpus of English text. It follows the tokenization conventions used in this dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T10:32:02.240424500Z",
     "start_time": "2023-10-15T10:32:02.229797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I', '.']\n",
      "['We', \"'re\", 'here', 'to', 'help', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer\n",
    "\n",
    "text1 = 'I have a Ph.D in A.I.'\n",
    "text2 = \"We're here to help mail us at nks@gmail.com\"\n",
    "text3 = 'A 5km ride cost $10.50'\n",
    "\n",
    "print(word_tokenize(text1))\n",
    "print(word_tokenize(text2))\n",
    "print(word_tokenize(text3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T10:52:31.481567100Z",
     "start_time": "2023-10-15T10:52:31.475071900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "['With these selections, players will take the reins of the Japanese Empire?',\n 'With dense rainforests to the north, mountains to the west, and the sea to the south, enemies will find it massively difficult to approach one`s starting location.',\n 'It`s a strong early-game position with a lot of potential to grow in peace.',\n 'Players won`t be left wanting for resources on this map, as there are a plethora of useful goodies scattered close together.',\n 'Initial access to the sea won`t be a problem either.',\n 'There`s also a nearby Natural Wonder that can be claimed in the first few turns, granting bonuses to adjacent tiles.']"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenizer\n",
    "\n",
    "sent_tokenize(text4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T10:36:43.639082600Z",
     "start_time": "2023-10-15T10:36:43.632931200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NLTK tokenization in Tweet data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "0                        [menyrbie philgahan chrisitv]\n1    [advice talk   neighbours family  exchange pho...\n2    [coronavirus australia woolworths  give elderl...\n3    [ food stock     one   empty please dont panic...\n4    [ ready  go  supermarket   covid19 outbreak   ...\nName: OriginalTweet, dtype: object"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweets = df['OriginalTweet'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "tokenized_tweets.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:42:31.727473100Z",
     "start_time": "2023-10-15T11:42:31.474002100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Using Spacy**\n",
    "\n",
    "**spaCy** is an open-source natural language processing (NLP) library designed for high-performance NLP tasks. It is known for its speed, accuracy, and ease of use. spaCy provides a wide range of NLP functionalities, including tokenization.\n",
    "\n",
    "In the context of tokenization, spaCy offers a tokenization component that is highly efficient and capable of handling multiple languages. spaCy's tokenizer not only splits text into words, but also handles more complex tokenization tasks, such as splitting off punctuation that appears at the beginning or end of a word.\n",
    "\n",
    "spaCy is widely used in both research and industry for various NLP tasks, including part-of-speech tagging, named entity recognition, dependency parsing, and more. Its efficient implementation and pre-trained models make it a popular choice for a wide range of NLP applications.\n",
    "\n",
    "spaCy uses an object-oriented approach and usually returns document objects with their own attributes and methods. Many users find spaCy to be more time and memory efficient than NLTK and therefore more suitable for production.\n",
    "\n",
    "`space.load()`: Function used to load a pre-trained spaCy model. It takes a model name as an argument and returns a loaded model object, which can be used for NLP tasks like tokenization, name entity recognition, dependency parsing, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "spacy.lang.en.English"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')   # ---> 'en_core_web_sm' is one of the pre-trained models (tokenizer) provided by spaCy for the English language. It is a small and efficient model that is suitable for a wide range of NLP tasks.\n",
    "\n",
    "type(nlp)    # <--- nlp has the data type Language, meaning that contains all components necessary for processing English text."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:59:18.910378300Z",
     "start_time": "2023-10-15T11:59:18.650269200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "doc3 = nlp(text3)\n",
    "\n",
    "for token in doc3:       # <--- Change the number of 'doc' to see tokenization of doc1 to doc3\n",
    "    print(token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:57:19.211139400Z",
     "start_time": "2023-10-15T11:57:19.198592500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Spacy tokenization in Tweet data** (It takes longer than the other tokenizers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "0                   [menyrbie, philgahan, chrisitv,  ]\n1    [advice, talk,   , neighbours, family,  , exch...\n2    [coronavirus, australia, woolworths,  , give, ...\n3    [ , food, stock,     , one,   , empty, please,...\n4    [ , ready,  , go,  , supermarket,   , covid19,...\nName: OriginalTweet, dtype: object"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]     # <--- token.text: Attribute containing the text of the token 'token'\n",
    "    return tokens\n",
    "\n",
    "tokenized_tweets = df['OriginalTweet'].apply(tokenize_text)\n",
    "\n",
    "tokenized_tweets.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T11:51:19.472334900Z",
     "start_time": "2023-10-15T11:48:19.658626700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['OriginalTweet'] == tokenized_tweets\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='4-3'>4.3 Stemming</a>\n",
    "\n",
    "*\"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood\"*\n",
    "\n",
    "\"**Stemming** *is the procesof reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language*\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Stemming** is a text processing technique in Natural Language Processing (NLP) that involves **reducing words to their base or root form**. The resulting form may not always be a valid word, but it can help in tasks like text analysis, information retrieval, and text mining.\n",
    "\n",
    "For example, the stem of the words \"running\", \"runner\", and \"ran\" is \"run\". By reducing these words to their common base form, we can treat them as the same word in terms of their core meaning.\n",
    "\n",
    "There are different algorithms for stemming, and one of the most widely used is the *Porter stemming algorithm*. The goal of stemming algorithms is **to strip affixes (prefixes and suffixes) from words to obtain their root form**.\n",
    "\n",
    "Stemming is useful in scenarios where you want to reduce the complexity of text data without being overly concerned about linguistic correctness. It's often used in tasks like information retrieval, search engines, and text mining, where reducing the variety of words can lead to more effective processing and analysis. However, **in contexts where linguistic precision is critical, more advanced techniques like lemmatization may be preferred**.\n",
    "\n",
    "Here are some key points about stemming:\n",
    "\n",
    "- *Reduces Dimensionality*: Stemming helps in reducing the dimensionality of the feature space in NLP tasks. It reduces the number of unique words or tokens in a text, making it easier to process.\n",
    "<br>\n",
    "\n",
    "- *Speeds Up Processing*: It can improve the efficiency of text processing tasks since it reduces the number of distinct words that need to be handled.\n",
    "<br>\n",
    "\n",
    "- *May Produce Non-Standard Words*: The resulting stems may not always be real words. For instance, the stem of \"jumps\" is \"jump\", but the stem of \"jumping\" is also \"jump\". This doesn't always align with standard English.\n",
    "<br>\n",
    "\n",
    "- *May Produce Ambiguous Results*: Stemming can sometimes produce stems that are ambiguous. For example, the stem of \"meeting\" could be either \"meet\" or \"meat\", depending on the context.\n",
    "<br>\n",
    "\n",
    "- *Less Contextually Sensitive*: Stemming is a rule-based approach and doesn't take context into account. It applies predefined rules to trim affixes, which means it might not always capture the correct root word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T13:03:55.486766400Z",
     "start_time": "2023-10-15T13:03:55.485747300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join(ps.stem(word) for word in text.split())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T13:05:44.817263700Z",
     "start_time": "2023-10-15T13:05:44.814098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk walk walk walk\n",
      "probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get\n"
     ]
    }
   ],
   "source": [
    "# Examples:\n",
    "# 1)\n",
    "sample = 'walk walks walking walked'\n",
    "print(stem_words(sample))\n",
    "\n",
    "# 2)\n",
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets'\n",
    "print(stem_words(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T13:09:24.170979300Z",
     "start_time": "2023-10-15T13:09:24.164757800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='4-4'>4.4 Lemmatization</a>\n",
    "\n",
    "\" **Lemmatization**, *unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called* **Lemma**.*A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Lemmatization** is a text processing technique in Natural Language Processing (NLP) that **involves reducing words to their base or root form, while still ensuring that the root form belongs to the language**. Unlike stemming, which may produce non-standard or even non-existent words, lemmatization ensures that the root word belongs to the language's dictionary.\n",
    "\n",
    "For example, the lemma (base form) of the words \"running\", \"runner\", and \"ran\" is \"run\". Similarly, the lemma of \"better\" is \"good\", and the lemma of \"meeting\" is \"meet\".\n",
    "\n",
    "Lemmatization is commonly used in applications where preserving linguistic correctness and semantic meaning is important, such as in machine translation, question-answering systems, chatbots, and other contexts where precise understanding of the text is crucial. It's also used in tasks that require word sense disambiguation, as it retains the original meaning of words.\n",
    "\n",
    "Here are some key points about lemmatization:\n",
    "\n",
    "- *Retains Valid Words*: Unlike stemming, lemmatization produces valid words that exist in the language's dictionary. This makes it more linguistically accurate.\n",
    "<br>\n",
    "\n",
    "- *Contextually Sensitive*: Lemmatization takes into account the context and part-of-speech (POS) of a word. For instance, the word \"better\" could be a comparative adjective or a verb. Lemmatization identifies the correct base form based on the context.\n",
    "<br>\n",
    "\n",
    "- *Slower Than Stemming*: Lemmatization is typically slower than stemming because it involves dictionary lookups and morphological analysis to determine the correct lemma.\n",
    "<br>\n",
    "\n",
    "- *Use of POS Tags*: Lemmatization often requires part-of-speech (POS) tags to accurately identify the lemma. For example, the lemma of \"better\" as an adjective is different from its lemma as a verb.\n",
    "<br>\n",
    "\n",
    "- *More Precise for Tasks Requiring Linguistic Accuracy*: Lemmatization is preferred in tasks where linguistic accuracy is critical, such as in language translation, sentiment analysis, or text summarization.\n",
    "<br>\n",
    "\n",
    "- *Larger Resource Requirements*: Lemmatization may require access to a larger linguistic resource, like a comprehensive dictionary or corpus, in order to perform accurately.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The **WordNetLemmatizer** is a class provided by the Natural Language Toolkit (NLTK) library in Python. It is **used for lemmatization**, which is the process of reducing words to their base or root form while ensuring that the resulting form belongs to the language's dictionary.\n",
    "\n",
    "Here are the key points about WordNetLemmatizer:\n",
    "\n",
    "- *Based on WordNet*: WordNet is a lexical database for the English language. It provides a hierarchy of words and their relationships, including synonyms, antonyms, and more. The WordNetLemmatizer uses WordNet as a reference to perform lemmatization.\n",
    "\n",
    "- *Linguistically Accurate*: The lemmas produced by the WordNetLemmatizer are valid words that exist in the English language. It ensures that the base form belongs to the language's dictionary.\n",
    "\n",
    "- *Contextually Sensitive*: It takes into account the context and part-of-speech (POS) of a word. Lemmatization is more accurate when accompanied by POS tagging, as different parts of speech may have different lemmatizations.\n",
    "\n",
    "- *Part of the NLTK Library*: The WordNetLemmatizer is part of the NLTK library, which is a widely-used library for natural language processing tasks in Python.\n",
    "\n",
    "- *Simple to Use*: It is easy to use. You instantiate the WordNetLemmatizer class and then apply the lemmatize() method to words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T20:18:08.985016700Z",
     "start_time": "2023-10-15T20:18:08.965296600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# Example 1:\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize a word\n",
    "lemma = wordnet_lemmatizer.lemmatize(\"running\", pos = \"v\")\n",
    "print(lemma)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T20:18:11.386217200Z",
     "start_time": "2023-10-15T20:18:11.382222100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# Example 2:\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Setting up the Sentence and Punctuations\n",
    "sentence = 'He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.'\n",
    "punctuations = '?:!.,;'\n",
    "\n",
    "# Tokenize the sentence into a list of words\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Remove the punctuation marks (puntuactions) from the list of words (sentence_words)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "sentence_words\n",
    "\n",
    "# Print the Words and their Lemmas\n",
    "print('{0:20}{1:20}'.format('Word','Lemma'))       # <--- This line prints the headers for the Word and Lemma columns, formatted to take up 20 characters each.\n",
    "\n",
    "for word in sentence_words:\n",
    "    print('{0:20}{1:20}'.format(word,wordnet_lemmatizer.lemmatize(word, pos = 'v')))     # <--- For each word, it prints the word and its lemma using wordnet_lemmatizer.lemmatize(word). The output is formatted to take up 20 characters for each column. The pos='v' argument specifies that the lemmatization should be performed assuming the word is a verb. This means it's finding the base form of each word assuming it's a verb."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T20:23:50.401887700Z",
     "start_time": "2023-10-15T20:23:50.368809300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Lemmatization with WordNetLemmatizer to Tweet Data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# Initialize the Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a Function for Lemmatization:\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([wordnet_lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n",
    "\n",
    "# Apply the Lemmatization Function to df:\n",
    "df['LemmatizedTweets'] = df['OriginalTweet'].apply(lemmatize_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T20:55:38.544608300Z",
     "start_time": "2023-10-15T20:55:37.029748300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "    UserName  ScreenName                   Location     TweetAt                                      OriginalTweet           Sentiment                                   LemmatizedTweets\n0       3799       48751                     London  16-03-2020                      menyrbie philgahan chrisitv               Neutral                        menyrbie philgahan chrisitv\n1       3800       48752                         UK  16-03-2020  advice talk   neighbours family  exchange phon...            Positive  advice talk neighbour family exchange phone nu...\n2       3801       48753                  Vagabonds  16-03-2020  coronavirus australia woolworths  give elderly...            Positive  coronavirus australia woolworths give elderly ...\n3       3802       48754                        NaN  16-03-2020   food stock     one   empty please dont panic ...            Positive  food stock one empty please dont panic enough ...\n4       3803       48755                        NaN  16-03-2020   ready  go  supermarket   covid19 outbreak   i...  Extremely Negative  ready go supermarket covid19 outbreak im paran...\n5       3804       48756   ÜT: 36.319708,-82.363649  16-03-2020   news   regionx0092s first confirmed covid19 c...            Positive  news regionx0092s first confirm covid19 case c...\n6       3805       48757       35.926541,-78.753267  16-03-2020  cashier  grocery store  sharing  insights  cov...            Positive  cashier grocery store share insights covid19 p...\n7       3806       48758                    Austria  16-03-2020     supermarket today didnt buy toilet paper re...             Neutral  supermarket today didnt buy toilet paper rebel...\n8       3807       48759            Atlanta, GA USA  16-03-2020  due  covid19  retail store  classroom  atlanta...            Positive  due covid19 retail store classroom atlanta ope...\n9       3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   corona preventionwe  stop  buy things   cash ...            Negative  corona preventionwe stop buy things cash use o...\n10      3809       48761             Makati, Manila  16-03-2020   month  hasnt  crowding   supermarkets  restau...             Neutral  month hasnt crowd supermarkets restaurants how...\n11      3810       48762  Pitt Meadows, BC, Canada   16-03-2020  due   covid19 situation   increased demand   f...  Extremely Positive  due covid19 situation increase demand food pro...\n12      3811       48763                 Horningsea  16-03-2020  horningsea   caring community letx0092s  look ...  Extremely Positive  horningsea care community letx0092s look less ...\n13      3812       48764                Chicago, IL  16-03-2020    dont need  stock   food ill   amazon deliver...            Positive  dont need stock food ill amazon deliver whatev...\n14      3813       48765                        NaN  16-03-2020  adara releases covid19 resource center  travel...            Positive  adara release covid19 resource center travel b...\n15      3814       48766             Houston, Texas  16-03-2020  lines   grocery store   unpredictable   eating...            Positive  line grocery store unpredictable eat safe alte...\n16      3815       48767               Saudi Arabia  16-03-2020                                                 13             Neutral                                                 13\n17      3816       48768            Ontario, Canada  16-03-2020  eyeonthearctic 16mar20 russia consumer surveil...             Neutral  eyeonthearctic 16mar20 russia consumer surveil...\n18      3817       48769              North America  16-03-2020  amazon glitch stymies whole foods fresh grocer...  Extremely Positive  amazon glitch stymy whole foods fresh grocery ...\n19      3818       48770                 Denver, CO  16-03-2020     arent struggling please consider donating  ...            Positive  arent struggle please consider donate food ban...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>UserName</th>\n      <th>ScreenName</th>\n      <th>Location</th>\n      <th>TweetAt</th>\n      <th>OriginalTweet</th>\n      <th>Sentiment</th>\n      <th>LemmatizedTweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3799</td>\n      <td>48751</td>\n      <td>London</td>\n      <td>16-03-2020</td>\n      <td>menyrbie philgahan chrisitv</td>\n      <td>Neutral</td>\n      <td>menyrbie philgahan chrisitv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3800</td>\n      <td>48752</td>\n      <td>UK</td>\n      <td>16-03-2020</td>\n      <td>advice talk   neighbours family  exchange phon...</td>\n      <td>Positive</td>\n      <td>advice talk neighbour family exchange phone nu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3801</td>\n      <td>48753</td>\n      <td>Vagabonds</td>\n      <td>16-03-2020</td>\n      <td>coronavirus australia woolworths  give elderly...</td>\n      <td>Positive</td>\n      <td>coronavirus australia woolworths give elderly ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3802</td>\n      <td>48754</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>food stock     one   empty please dont panic ...</td>\n      <td>Positive</td>\n      <td>food stock one empty please dont panic enough ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3803</td>\n      <td>48755</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>ready  go  supermarket   covid19 outbreak   i...</td>\n      <td>Extremely Negative</td>\n      <td>ready go supermarket covid19 outbreak im paran...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3804</td>\n      <td>48756</td>\n      <td>ÜT: 36.319708,-82.363649</td>\n      <td>16-03-2020</td>\n      <td>news   regionx0092s first confirmed covid19 c...</td>\n      <td>Positive</td>\n      <td>news regionx0092s first confirm covid19 case c...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3805</td>\n      <td>48757</td>\n      <td>35.926541,-78.753267</td>\n      <td>16-03-2020</td>\n      <td>cashier  grocery store  sharing  insights  cov...</td>\n      <td>Positive</td>\n      <td>cashier grocery store share insights covid19 p...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3806</td>\n      <td>48758</td>\n      <td>Austria</td>\n      <td>16-03-2020</td>\n      <td>supermarket today didnt buy toilet paper re...</td>\n      <td>Neutral</td>\n      <td>supermarket today didnt buy toilet paper rebel...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3807</td>\n      <td>48759</td>\n      <td>Atlanta, GA USA</td>\n      <td>16-03-2020</td>\n      <td>due  covid19  retail store  classroom  atlanta...</td>\n      <td>Positive</td>\n      <td>due covid19 retail store classroom atlanta ope...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3808</td>\n      <td>48760</td>\n      <td>BHAVNAGAR,GUJRAT</td>\n      <td>16-03-2020</td>\n      <td>corona preventionwe  stop  buy things   cash ...</td>\n      <td>Negative</td>\n      <td>corona preventionwe stop buy things cash use o...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3809</td>\n      <td>48761</td>\n      <td>Makati, Manila</td>\n      <td>16-03-2020</td>\n      <td>month  hasnt  crowding   supermarkets  restau...</td>\n      <td>Neutral</td>\n      <td>month hasnt crowd supermarkets restaurants how...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>3810</td>\n      <td>48762</td>\n      <td>Pitt Meadows, BC, Canada</td>\n      <td>16-03-2020</td>\n      <td>due   covid19 situation   increased demand   f...</td>\n      <td>Extremely Positive</td>\n      <td>due covid19 situation increase demand food pro...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>3811</td>\n      <td>48763</td>\n      <td>Horningsea</td>\n      <td>16-03-2020</td>\n      <td>horningsea   caring community letx0092s  look ...</td>\n      <td>Extremely Positive</td>\n      <td>horningsea care community letx0092s look less ...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3812</td>\n      <td>48764</td>\n      <td>Chicago, IL</td>\n      <td>16-03-2020</td>\n      <td>dont need  stock   food ill   amazon deliver...</td>\n      <td>Positive</td>\n      <td>dont need stock food ill amazon deliver whatev...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3813</td>\n      <td>48765</td>\n      <td>NaN</td>\n      <td>16-03-2020</td>\n      <td>adara releases covid19 resource center  travel...</td>\n      <td>Positive</td>\n      <td>adara release covid19 resource center travel b...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3814</td>\n      <td>48766</td>\n      <td>Houston, Texas</td>\n      <td>16-03-2020</td>\n      <td>lines   grocery store   unpredictable   eating...</td>\n      <td>Positive</td>\n      <td>line grocery store unpredictable eat safe alte...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>3815</td>\n      <td>48767</td>\n      <td>Saudi Arabia</td>\n      <td>16-03-2020</td>\n      <td>13</td>\n      <td>Neutral</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>3816</td>\n      <td>48768</td>\n      <td>Ontario, Canada</td>\n      <td>16-03-2020</td>\n      <td>eyeonthearctic 16mar20 russia consumer surveil...</td>\n      <td>Neutral</td>\n      <td>eyeonthearctic 16mar20 russia consumer surveil...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>3817</td>\n      <td>48769</td>\n      <td>North America</td>\n      <td>16-03-2020</td>\n      <td>amazon glitch stymies whole foods fresh grocer...</td>\n      <td>Extremely Positive</td>\n      <td>amazon glitch stymy whole foods fresh grocery ...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>3818</td>\n      <td>48770</td>\n      <td>Denver, CO</td>\n      <td>16-03-2020</td>\n      <td>arent struggling please consider donating  ...</td>\n      <td>Positive</td>\n      <td>arent struggle please consider donate food ban...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T20:55:53.717057Z",
     "start_time": "2023-10-15T20:55:53.687674400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='5'>5 Train-Test Split of Data</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32925, 6)\n",
      "(32925,)\n",
      "(8232, 6)\n",
      "(8232,)\n"
     ]
    }
   ],
   "source": [
    "# SPLIT DATA\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop('Sentiment', axis = 1),\n",
    "                                                    df['Sentiment'],\n",
    "                                                    train_size=0.8,                            # <--- 80% train and 20% test\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T21:01:18.539738300Z",
     "start_time": "2023-10-15T21:01:18.432946100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a id='6'>6 Feature Extraction from Text</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Feature Extraction from Text** involves **converting raw text data into a numerical format** that can be used as input for machine learning models. This is a crucial step in natural language processing (NLP) tasks, as most machine learning algorithms require numerical input. The extracted features are then used as input for machine learning models to perform tasks like classification, regression, clustering, and more. The choice of feature extraction technique depends on the specific NLP task, the nature of the data, and the characteristics of the text corpus.\n",
    "\n",
    "Here are **some common techniques** for feature extraction from text:\n",
    "\n",
    "- *Bag of Words (BoW)*:\n",
    "    - BoW represents text data as a collection of words, disregarding grammar and word order. It creates a vocabulary of all unique words in a corpus and counts the frequency of each word in a document. Each document is then represented as a vector where each element corresponds to the frequency of a word in the vocabulary.\n",
    "<br>\n",
    "\n",
    "- *Term Frequency-Inverse Document Frequency (TF-IDF)*:\n",
    "    - TF-IDF is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It takes into account both the frequency of a word in a document (Term Frequency) and the rarity of the word in the entire corpus (Inverse Document Frequency).\n",
    "<br>\n",
    "\n",
    "- *Word Embeddings*:\n",
    "    - Word embeddings are dense, low-dimensional vectors that represent words in a continuous vector space. Techniques like Word2Vec, GloVe, and FastText learn these embeddings by considering the context in which words appear. They capture semantic relationships between words and are effective in capturing word similarity and analogy.\n",
    "<br>\n",
    "\n",
    "- *Word Counts and Character Counts*:\n",
    "    - Simple features like the total number of words in a document, average word length, or frequency of specific characters can also be used as features.\n",
    "<br>\n",
    "\n",
    "- *N-grams*:\n",
    "    - N-grams are sequences of $N$ consecutive words. For example, Bi-grams consist of pairs of adjacent words. By considering sequences of words, N-grams can capture more context compared to BoW.\n",
    "<br>\n",
    "\n",
    "- *Part-of-Speech (POS) Tagging*:\n",
    "    - POS tagging assigns a grammatical label to each word in a sentence (e.g., noun, verb, adjective). These tags can be used as features to capture linguistic information.\n",
    "<br>\n",
    "\n",
    "- *Sentiment Scores*:\n",
    "    - Sentiment analysis tools can be used to assign sentiment scores to text, indicating the sentiment (positive, negative, neutral) expressed in the text.\n",
    "<br>\n",
    "\n",
    "- *Topic Modeling*:\n",
    "    - Topic modeling techniques like Latent Dirichlet Allocation (LDA) can be used to extract topics from a collection of documents. The distribution of topics in a document can be used as features.\n",
    "<br>\n",
    "\n",
    "- *Syntactic Features*:\n",
    "    - Features related to sentence structure, such as the presence of specific grammatical constructs (e.g., passive voice, conditional clauses), can be used.\n",
    "<br>\n",
    "\n",
    "- *Dependency Parsing*:\n",
    "    - Features based on syntactic relationships between words in a sentence can be used to capture structural information.\n",
    "<br>\n",
    "\n",
    "- *Lexical Diversity Measures*:\n",
    "    - Metrics like type-token ratio or TTR (ratio of unique words to total words) can be used to measure the richness and diversity of vocabulary in a document.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <a id='6-1'>6.1 Bags of Words</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The Bag of Words (BoW)** method is a fundamental technique in Natural Language Processing (NLP) used for text processing and feature extraction. It's called \"bag\" because it **involves treating text data as an unordered collection or bag of words, disregarding grammar, word order, and context**. BoW is widely used in various NLP tasks like sentiment analysis, document classification, and information retrieval.\n",
    "\n",
    "Here are the **key steps** and concepts in the Bag of Words method:\n",
    "\n",
    "- *Tokenization*:\n",
    "    - The first step is to break down a piece of text into individual words or tokens. This process may involve removing punctuation and handling special cases like contractions.\n",
    "\n",
    "- *Vocabulary Building*:\n",
    "    - Once the text is tokenized, a vocabulary is constructed. This vocabulary consists of all unique words (or tokens) that appear in the corpus (collection of documents).\n",
    "\n",
    "- *Word Frequency Count*:\n",
    "    - For each document in the corpus, a vector is created where each element represents the frequency of a word in the document. These vectors can be very high-dimensional, with each dimension corresponding to a word in the vocabulary.\n",
    "\n",
    "- *Sparse Matrix Representation*:\n",
    "    - The result of BoW is often represented as a sparse matrix. A sparse matrix is a data structure that only stores non-zero elements, which are the counts of words in this case. This is efficient in terms of memory.\n",
    "\n",
    "- *Normalization (Optional)*:\n",
    "    - Depending on the specific task, the frequency counts can be normalized to make them more comparable across different documents. Common normalization techniques include TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "- *Feature Vectors*:\n",
    "    - Each document is represented as a feature vector where each element corresponds to the frequency of a specific word in the vocabulary. The order of the words does not matter, hence the term \"bag of words\".\n",
    "\n",
    "- *Loss of Contextual Information*:\n",
    "    - One limitation of BoW is that it completely ignores the order of words and any contextual information. For example, \"not good\" and \"good not\" would be represented the same way.\n",
    "\n",
    "- *High Dimensionality*:\n",
    "    - BoW can lead to high-dimensional feature spaces, especially for large vocabularies and extensive documents. This can impact the efficiency of some machine learning algorithms.\n",
    "\n",
    "- *Application in Machine Learning*:\n",
    "    - BoW vectors are commonly used as input features for various machine learning models. For example, in sentiment analysis, these vectors can be fed into a classifier to predict the sentiment of a document.\n",
    "\n",
    "<u>Note: </u>\n",
    "BoW is a powerful and versatile technique, but it may not be suitable for tasks where word order or context is crucial (e.g., language translation or tasks requiring understanding of semantics). In such cases, more advanced techniques like word embeddings or deep learning models may be more appropriate."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
